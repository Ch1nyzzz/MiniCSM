# MiniCSM

This project implements a knowledge distillation framework based on Hugging Face, extracting knowledge from the teacher model to train a smaller and more deployable student model. It is suitable for scenarios where deploying large-scale language models under limited resources is required. 
The base model is a fine-tuned T5-xxl designed for soft contextual adjustment on social media platforms.
---

## **Code introduction**
- intermediate layer alignment has been utilized

